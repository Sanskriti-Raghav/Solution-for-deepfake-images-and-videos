{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, glob\n",
        "\n",
        "print(\"All model files I can see under MyDrive:\\n\")\n",
        "for p in glob.glob(\"/content/drive/MyDrive/**/*\", recursive=True):\n",
        "    if p.endswith(\".pth\") or p.endswith(\".pt\") or p.endswith(\".safetensors\"):\n",
        "        print(p)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8nP7WAMv9zj",
        "outputId": "54e15fa0-c60e-4ea2-b6aa-372bc09339d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "All model files I can see under MyDrive:\n",
            "\n",
            "/content/drive/MyDrive/Deepfake/best_fusion_model.pth\n",
            "/content/drive/MyDrive/Deepfake/deit_fusion_epoch5.pth\n",
            "/content/drive/MyDrive/Deepfake/early_fusion_model/best_mlp_state.pth\n",
            "/content/drive/MyDrive/Deepfake/early_fusion_model/early_fusion_mlp_final.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Install packages (one time per session) ----\n",
        "import subprocess, sys\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                       \"transformers\", \"timm\", \"tqdm\", \"onnxruntime\",\n",
        "                       \"gradio\", \"dlib\", \"opencv-python\"])\n",
        "\n",
        "# -------------------- Imports & config --------------------\n",
        "import os, cv2, dlib\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoImageProcessor, DeiTModel\n",
        "import gradio as gr\n",
        "\n",
        "IMG_SIZE     = 256\n",
        "SYM_DIM      = 50\n",
        "NUM_CLASSES  = 2\n",
        "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "WEIGHT_PATH  = \"/content/drive/MyDrive/Deepfake/deit_fusion_epoch5.pth\"\n",
        "LANDMARKS    = \"shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "# -------------------- Landmarks download --------------------\n",
        "if not os.path.exists(LANDMARKS):\n",
        "    print(\"Downloading landmark predictor...\")\n",
        "    os.system(\"wget -q http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n",
        "    os.system(\"bunzip2 -f shape_predictor_68_face_landmarks.dat.bz2\")\n",
        "    print(\"‚úì Landmarks downloaded\")\n",
        "\n",
        "# -------------------- Symmetry extractor --------------------\n",
        "class FacialSymmetryExtractor:\n",
        "    def __init__(self, landmarks=LANDMARKS, dim=SYM_DIM):\n",
        "        self.detector  = dlib.get_frontal_face_detector()\n",
        "        self.predictor = dlib.shape_predictor(landmarks)\n",
        "        self.dim       = dim\n",
        "\n",
        "    def calc(self, pts):\n",
        "        cx = np.mean(pts[:, 0])\n",
        "        pairs = [\n",
        "            (0,16),(1,15),(2,14),(3,13),(4,12),(5,11),(6,10),(7,9),\n",
        "            (17,26),(18,25),(19,24),(20,23),(21,22),\n",
        "            (36,45),(37,44),(38,43),(39,42),(40,47),(41,46),\n",
        "            (31,35),(32,34),(48,54),(49,53),(50,52),(58,56),(59,55)\n",
        "        ]\n",
        "        feats = []\n",
        "        for l, r in pairs:\n",
        "            ld = abs(pts[l, 0] - cx)\n",
        "            rd = abs(pts[r, 0] - cx)\n",
        "            feats.extend([ld / (rd + 1e-6), abs(pts[l, 1] - pts[r, 1])])\n",
        "\n",
        "        le, re = pts[36:42], pts[42:48]\n",
        "        lw, rw = np.linalg.norm(le[3] - le[0]), np.linalg.norm(re[3] - re[0])\n",
        "        lh, rh = np.linalg.norm(le[1] - le[5]), np.linalg.norm(re[1] - re[5])\n",
        "        feats.extend([lw / (rw + 1e-6), lh / (rh + 1e-6)])\n",
        "\n",
        "        if len(feats) < self.dim:\n",
        "            feats.extend([0] * (self.dim - len(feats)))\n",
        "        return np.array(feats[:self.dim], dtype=np.float32)\n",
        "\n",
        "    def extract(self, arr):\n",
        "        try:\n",
        "            gray = cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)\n",
        "        except:\n",
        "            gray = cv2.cvtColor(arr, cv2.COLOR_BGR2GRAY)\n",
        "        faces = self.detector(gray)\n",
        "        if not faces:\n",
        "            return np.zeros(self.dim, dtype=np.float32)\n",
        "        largest = max(faces, key=lambda f: f.width() * f.height())\n",
        "        lm = self.predictor(gray, largest)\n",
        "        pts = np.array([[lm.part(i).x, lm.part(i).y] for i in range(68)], dtype=np.float32)\n",
        "        return self.calc(pts)\n",
        "\n",
        "sym_extractor = FacialSymmetryExtractor()\n",
        "print(\"Symmetry extractor ready. SYM_DIM =\", SYM_DIM)\n",
        "\n",
        "# -------------------- Processor & model --------------------\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/deit-small-patch16-224\")\n",
        "\n",
        "class EarlyFusionHF(nn.Module):\n",
        "    def __init__(self, deit_model, sym_dim=SYM_DIM, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.deit = deit_model\n",
        "        self.fc   = nn.Linear(384 + sym_dim, num_classes)  # DeiT-small CLS dim = 384\n",
        "\n",
        "    def forward(self, x, sym):\n",
        "        outputs = self.deit(x)\n",
        "        cls_tok = outputs.last_hidden_state[:, 0, :]\n",
        "        fused   = torch.cat([cls_tok, sym], dim=1)\n",
        "        return self.fc(fused)\n",
        "\n",
        "print(\"Loading DeiT backbone...\")\n",
        "deit_model = DeiTModel.from_pretrained(\"facebook/deit-small-patch16-224\").to(DEVICE)\n",
        "deit_model.eval()\n",
        "\n",
        "model = EarlyFusionHF(deit_model, sym_dim=SYM_DIM, num_classes=NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "if not os.path.exists(WEIGHT_PATH):\n",
        "    raise FileNotFoundError(f\"Checkpoint not found: {WEIGHT_PATH}\")\n",
        "\n",
        "state = torch.load(WEIGHT_PATH, map_location=DEVICE)\n",
        "model.load_state_dict(state)\n",
        "model.eval()\n",
        "print(\"‚úì Trained model loaded from\", WEIGHT_PATH)\n",
        "\n",
        "GLOBAL_MODEL = model\n",
        "\n",
        "# -------------------- Prediction logic --------------------\n",
        "def predict_from_pil(pil_image):\n",
        "    if pil_image is None:\n",
        "        return \"ERROR\", 0.0, {\"error\": \"No image\"}\n",
        "    try:\n",
        "        img_rgb = np.array(pil_image.convert(\"RGB\"))\n",
        "        sym = sym_extractor.extract(img_rgb)\n",
        "        sym_t = torch.tensor(sym, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        inputs = processor(images=pil_image, return_tensors=\"pt\")\n",
        "        x = inputs[\"pixel_values\"].to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out   = GLOBAL_MODEL(x, sym_t)\n",
        "            probs = F.softmax(out.squeeze(0), dim=0).cpu().numpy()\n",
        "        pred  = int(out.argmax(1).item())\n",
        "        label = \"FAKE\" if pred == 1 else \"REAL\"\n",
        "        conf  = float(probs.max())\n",
        "\n",
        "        sym_score = float(np.mean(np.abs(sym)))\n",
        "\n",
        "        if label == \"FAKE\":\n",
        "            reason = (\n",
        "                \"The model detected patterns consistent with manipulation, including \"\n",
        "                \"irregular facial symmetry and atypical texture patterns.\"\n",
        "            )\n",
        "        else:\n",
        "            reason = (\n",
        "                \"Facial symmetry and texture patterns look consistent with typical real images, \"\n",
        "                \"and no strong manipulation cues were found.\"\n",
        "            )\n",
        "\n",
        "        details = {\n",
        "            \"confidence\": conf,\n",
        "            \"symmetry_score\": sym_score,\n",
        "            \"reason\": reason,\n",
        "        }\n",
        "        return label, conf, details\n",
        "    except Exception as e:\n",
        "        return \"ERROR\", 0.0, {\"error\": str(e)}\n",
        "\n",
        "# -------------------- Pair analysis helper --------------------\n",
        "def analyse_pair(image_a, image_b):\n",
        "    if image_a is None or image_b is None:\n",
        "        return (\n",
        "            gr.update(\n",
        "                value=\"‚ùå Please upload both Image A and Image B.\",\n",
        "                visible=True,\n",
        "            ),\n",
        "            gr.update(value=\"\", visible=False),\n",
        "            gr.update(value=\"\", visible=False),\n",
        "        )\n",
        "\n",
        "    label_a, conf_a, details_a = predict_from_pil(image_a)\n",
        "    label_b, conf_b, details_b = predict_from_pil(image_b)\n",
        "\n",
        "    if label_a == \"ERROR\":\n",
        "        return (\n",
        "            gr.update(value=f\"Error on Image A: {details_a.get('error','Unknown')}\", visible=True),\n",
        "            gr.update(value=\"\", visible=False),\n",
        "            gr.update(value=\"\", visible=False),\n",
        "        )\n",
        "    if label_b == \"ERROR\":\n",
        "        return (\n",
        "            gr.update(value=f\"Error on Image B: {details_b.get('error','Unknown')}\", visible=True),\n",
        "            gr.update(value=\"\", visible=False),\n",
        "            gr.update(value=\"\", visible=False),\n",
        "        )\n",
        "\n",
        "    conf_a_pct = details_a[\"confidence\"] * 100.0\n",
        "    conf_b_pct = details_b[\"confidence\"] * 100.0\n",
        "\n",
        "    combo = f\"{label_a.upper()} + {label_b.upper()}\"\n",
        "    if label_a == \"REAL\" and label_b == \"FAKE\":\n",
        "        combo_expl = \"Image A looks authentic, while Image B shows strong manipulation cues.\"\n",
        "    elif label_a == \"FAKE\" and label_b == \"REAL\":\n",
        "        combo_expl = \"Image A appears manipulated, while Image B looks authentic.\"\n",
        "    elif label_a == \"FAKE\" and label_b == \"FAKE\":\n",
        "        combo_expl = \"Both images are likely deepfakes, possibly generated or edited independently.\"\n",
        "    else:\n",
        "        combo_expl = \"Both images look real; no strong deepfake artefacts were detected.\"\n",
        "\n",
        "    summary_md = f\"\"\"### üß© Pair summary\n",
        "\n",
        "- **Combination:** `{combo}`\n",
        "- **Explanation:** {combo_expl}\n",
        "\"\"\"\n",
        "\n",
        "    details_md = f\"\"\"### üñºÔ∏è Image A\n",
        "\n",
        "- **Prediction:** `{label_a}`\n",
        "- **Confidence:** {conf_a_pct:.2f}%\n",
        "- **Symmetry score:** {details_a.get(\"symmetry_score\",0):.2f}\n",
        "\n",
        "---\n",
        "\n",
        "### üñºÔ∏è Image B\n",
        "\n",
        "- **Prediction:** `{label_b}`\n",
        "- **Confidence:** {conf_b_pct:.2f}%\n",
        "- **Symmetry score:** {details_b.get(\"symmetry_score\",0):.2f}\n",
        "\"\"\"\n",
        "\n",
        "    return (\n",
        "        gr.update(value=\"\", visible=False),\n",
        "        gr.update(value=summary_md, visible=True),\n",
        "        gr.update(value=details_md, visible=True),\n",
        "    )\n",
        "\n",
        "# -------------------- Gradio UI --------------------\n",
        "custom_css = \"\"\"\n",
        ".gradio-container {max-width: 1100px !important; font-family: 'Inter', system-ui, sans-serif;}\n",
        "* {box-sizing: border-box;}\n",
        ".main-card {\n",
        "  background: #020617;\n",
        "  border-radius: 26px;\n",
        "  border: 1px solid rgba(148,163,184,0.45);\n",
        "  padding: 28px 32px;\n",
        "  box-shadow: 0 18px 45px rgba(15,23,42,0.75);\n",
        "}\n",
        ".section-title {\n",
        "  font-size: 2.4rem;\n",
        "  font-weight: 800;\n",
        "  text-align: center;\n",
        "  color: #c4b5fd;\n",
        "  margin-bottom: 0.3rem;\n",
        "}\n",
        ".section-subtitle {\n",
        "  text-align: center;\n",
        "  font-size: 1.02rem;\n",
        "  color:#9ca3af;\n",
        "  margin-bottom: 2.2rem;\n",
        "}\n",
        ".chip {\n",
        "  display:inline-flex;\n",
        "  align-items:center;\n",
        "  gap:8px;\n",
        "  padding:4px 10px;\n",
        "  border-radius:999px;\n",
        "  background:#0f172a;\n",
        "  border:1px solid rgba(148,163,184,0.5);\n",
        "  color:#e5e7eb;\n",
        "  font-size:0.78rem;\n",
        "}\n",
        ".pill-button {\n",
        "  border-radius:999px !important;\n",
        "  font-weight:600 !important;\n",
        "  letter-spacing:0.02em;\n",
        "  border:none !important;\n",
        "  background: linear-gradient(90deg,#6366f1,#a855f7,#ec4899) !important;\n",
        "  color:white !important;\n",
        "  padding:12px 26px !important;\n",
        "}\n",
        ".pill-button:hover {\n",
        "  box-shadow: 0 0 25px rgba(129,140,248,0.7);\n",
        "  transform: translateY(-1px);\n",
        "  transition: all 0.12s ease-out;\n",
        "}\n",
        ".secondary-pill {\n",
        "  border-radius:999px !important;\n",
        "  font-weight:500 !important;\n",
        "  border:1px solid rgba(148,163,184,0.5) !important;\n",
        "  background:rgba(15,23,42,0.9) !important;\n",
        "  color:#e5e7eb !important;\n",
        "}\n",
        ".upload-box {\n",
        "  border:2px dashed rgba(148,163,184,0.8) !important;\n",
        "  border-radius:20px !important;\n",
        "  background:rgba(15,23,42,0.8) !important;\n",
        "}\n",
        ".preview-panel {\n",
        "  background: radial-gradient(circle at top, #0b1120, #020617);\n",
        "  border-radius:20px;\n",
        "  border:1px solid rgba(148,163,184,0.5);\n",
        "  padding:10px;\n",
        "}\n",
        ".result-banner {\n",
        "  border-radius:22px;\n",
        "  padding:24px 26px;\n",
        "  margin-bottom:18px;\n",
        "}\n",
        ".result-badge {\n",
        "  display:inline-flex;\n",
        "  align-items:center;\n",
        "  justify-content:center;\n",
        "  width:74px;\n",
        "  height:74px;\n",
        "  border-radius:22px;\n",
        "  background:#0b1120;\n",
        "  border:2px solid rgba(248,113,113,0.8);\n",
        "  color:#fecaca;\n",
        "  font-size:2rem;\n",
        "}\n",
        ".result-title {\n",
        "  font-size:2.0rem;\n",
        "  font-weight:800;\n",
        "  letter-spacing:0.03em;\n",
        "}\n",
        ".result-sub {\n",
        "  font-size:0.98rem;\n",
        "  color:#e5e7eb;\n",
        "}\n",
        ".score-box {\n",
        "  background:#020617;\n",
        "  border-radius:16px;\n",
        "  border:1px solid rgba(148,163,184,0.6);\n",
        "  padding:14px 16px;\n",
        "  font-size:0.9rem;\n",
        "  color:#e5e7eb;\n",
        "}\n",
        ".score-label {\n",
        "  color:#9ca3af;\n",
        "  font-size:0.8rem;\n",
        "  text-transform:uppercase;\n",
        "  letter-spacing:0.06em;\n",
        "}\n",
        ".score-value {\n",
        "  font-size:1.02rem;\n",
        "  font-weight:600;\n",
        "}\n",
        ".prob-row {\n",
        "  display:flex;\n",
        "  justify-content:space-between;\n",
        "  font-size:0.88rem;\n",
        "}\n",
        ".details-box {\n",
        "  border-radius: 16px !important;\n",
        "  border: 1px solid rgba(79,70,229,0.5) !important;\n",
        "  background: radial-gradient(circle at top left, rgba(79,70,229,0.2), rgba(15,23,42,1)) !important;\n",
        "  padding: 1.1rem 1.2rem !important;\n",
        "  color:#e5e7eb;\n",
        "}\n",
        ".error-box {color:#fecaca;}\n",
        "\n",
        "/* Glow around compare images */\n",
        ".main-card .gradio-image {\n",
        "  border-radius: 18px !important;\n",
        "  box-shadow: 0 0 25px rgba(56,189,248,0.25);\n",
        "}\n",
        ".main-card .gradio-image:hover {\n",
        "  box-shadow: 0 0 35px rgba(129,140,248,0.45);\n",
        "  transition: box-shadow 0.12s ease-out;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def go_tab(idx):  # 0:Home 1:Upload 2:Compare 3:Result 4:About\n",
        "    return gr.Tabs(selected=idx)\n",
        "\n",
        "def handle_upload(filepath):\n",
        "    if filepath is None:\n",
        "        return (gr.update(value=\"‚ùå Please upload a JPG or PNG image.\", visible=True),\n",
        "                None,\n",
        "                gr.update(interactive=False),\n",
        "                gr.update(visible=False),\n",
        "                \"\")\n",
        "    try:\n",
        "        pilimg = Image.open(filepath).convert(\"RGB\")\n",
        "        info = f\"Size: {pilimg.width}√ó{pilimg.height} | Format: {pilimg.format or 'Unknown'}\"\n",
        "        return (\n",
        "            gr.update(value=\"\", visible=False),\n",
        "            pilimg,\n",
        "            gr.update(interactive=True),\n",
        "            gr.update(visible=True),\n",
        "            info,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return (gr.update(value=f\"‚ùå Error: {e}\", visible=True),\n",
        "                None,\n",
        "                gr.update(interactive=False),\n",
        "                gr.update(visible=False),\n",
        "                \"\")\n",
        "\n",
        "def analyse_and_go(image):\n",
        "    if image is None:\n",
        "        return (\"Please upload an image first.\", \"\", \"\", \"\", go_tab(1))\n",
        "\n",
        "    label, conf, details = predict_from_pil(image)\n",
        "    if label == \"ERROR\":\n",
        "        return (f\"Processing error: {details.get('error','Unknown')}\", \"\", \"\", \"\", go_tab(1))\n",
        "\n",
        "    is_fake = (label == \"FAKE\")\n",
        "    conf_pct = conf * 100.0\n",
        "    sym = details.get(\"symmetry_score\", 0.0)\n",
        "    reason = details.get(\"reason\", \"\")\n",
        "\n",
        "    if is_fake:\n",
        "        banner_color = \"rgba(127,29,29,0.85)\"\n",
        "        text_color   = \"#fecaca\"\n",
        "        title_text   = \"DEEPFAKE DETECTED\"\n",
        "        sub_text     = \"This image appears to be AI‚Äëgenerated or manipulated.\"\n",
        "        icon         = \"‚õî\"\n",
        "    else:\n",
        "        banner_color = \"rgba(22,101,52,0.90)\"\n",
        "        text_color   = \"#bbf7d0\"\n",
        "        title_text   = \"LIKELY REAL\"\n",
        "        sub_text     = \"No strong evidence of manipulation was detected.\"\n",
        "        icon         = \"‚úÖ\"\n",
        "\n",
        "    banner_html = f\"\"\"\n",
        "    <div class=\"result-banner\" style=\"background:{banner_color}; border:1px solid rgba(15,23,42,0.8);\">\n",
        "      <div style=\"display:flex; gap:20px; align-items:center;\">\n",
        "        <div class=\"result-badge\">{icon}</div>\n",
        "        <div>\n",
        "          <div class=\"result-title\" style=\"color:{text_color};\">{title_text}</div>\n",
        "          <div class=\"result-sub\">{sub_text}</div>\n",
        "        </div>\n",
        "      </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Decide probabilities based on is_fake\n",
        "    if is_fake:\n",
        "        real_pct = (1 - conf) * 100\n",
        "        fake_pct = conf_pct\n",
        "    else:\n",
        "        real_pct = conf_pct\n",
        "        fake_pct = (1 - conf) * 100\n",
        "\n",
        "    score_html = f\"\"\"\n",
        "  <div style=\"display:flex; gap:16px; margin-bottom:14px;\">\n",
        "    <div class=\"score-box\" style=\"flex:1;\">\n",
        "      <div class=\"score-label\">Confidence</div>\n",
        "      <div class=\"score-value\">{conf_pct:.1f}%</div>\n",
        "    </div>\n",
        "    <div class=\"score-box\" style=\"flex:1;\">\n",
        "      <div class=\"score-label\">Probabilities</div>\n",
        "      <div class=\"prob-row\">\n",
        "        <span>Real:</span><span>{real_pct:.1f}%</span>\n",
        "      </div>\n",
        "      <div class=\"prob-row\">\n",
        "        <span>Fake:</span><span>{fake_pct:.1f}%</span>\n",
        "      </div>\n",
        "    </div>\n",
        "  </div>\n",
        "  <div style=\"margin:10px 0 4px; font-size:0.86rem; color:#9ca3af;\">\n",
        "    Model: DeiT Vision Transformer + Facial Symmetry Features\n",
        "  </div>\n",
        "  <div style=\"width:100%; height:10px; border-radius:999px; background:#111827; overflow:hidden;\">\n",
        "    <div style=\"width:{conf_pct}%; height:100%;\n",
        "                background:linear-gradient(90deg,#22c55e,#eab308,#f97316,#ef4444);\"></div>\n",
        "  </div>\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "    explain_md = f\"\"\"\n",
        "### üß† Why this result?\n",
        "\n",
        "- **Decision:** `{label}` with **{conf_pct:.2f}%** confidence\n",
        "- **Symmetry analysis:** average symmetry score **{sym:.2f}** (higher = more asymmetry)\n",
        "- **Reasoning:** {reason}\n",
        "\n",
        "> Note: This is an AI prediction. Always verify with multiple methods and human review.\n",
        "\"\"\"\n",
        "\n",
        "    return (\"\", banner_html, score_html, explain_md, go_tab(3))\n",
        "\n",
        "# -------------------- Build tabs --------------------\n",
        "with gr.Blocks(title=\"Deepfake Detection\", css=custom_css) as demo:\n",
        "    with gr.Tabs(selected=0) as tabs:\n",
        "\n",
        "        # PAGE 1 ‚Äì Home (centered hero + floating panel)\n",
        "        with gr.Tab(\"Home\", id=0):\n",
        "            with gr.Column(elem_classes=\"main-card\"):\n",
        "                gr.HTML(\n",
        "                    \"\"\"\n",
        "<div style=\"display:flex; flex-direction:column; align-items:center; text-align:center; gap:26px;\">\n",
        "  <div style=\"max-width:720px;\">\n",
        "    <div style=\"font-size:0.9rem; text-transform:uppercase; letter-spacing:0.24em;\n",
        "                color:#a5b4fc; margin-bottom:10px;\">\n",
        "      üîí DEEPFAKE DETECTION\n",
        "    </div>\n",
        "    <div style=\"font-size:2.7rem; font-weight:800; line-height:1.15; color:#e5e7eb;\">\n",
        "      Analyze face images for<br/>\n",
        "      <span style=\"color:#c4b5fd;\">AI‚Äëgenerated manipulation</span>\n",
        "    </div>\n",
        "    <p style=\"margin-top:16px; font-size:0.98rem; color:#9ca3af;\">\n",
        "      Upload a face photo and this tool will estimate whether it is likely real or a deepfake,\n",
        "      using transformer features and facial symmetry cues.\n",
        "    </p>\n",
        "\n",
        "    <div style=\"display:flex; flex-wrap:wrap; justify-content:center;\n",
        "                gap:10px; margin:18px 0 8px;\">\n",
        "      <div class=\"chip\">üéØ Deepfake probability score</div>\n",
        "      <div class=\"chip\">‚ö° Results in under a second</div>\n",
        "      <div class=\"chip\">üß† Symmetry‚Äëaware transformer</div>\n",
        "    </div>\n",
        "\n",
        "    <div style=\"margin-top:18px; display:flex; justify-content:center;\">\n",
        "\"\"\"\n",
        "                )\n",
        "                start_btn = gr.Button(\"üöÄ START ANALYZING\", elem_classes=\"pill-button\")\n",
        "                gr.HTML(\n",
        "                    \"\"\"\n",
        "    </div>\n",
        "\n",
        "    <div style=\"margin-top:18px; font-size:0.82rem; color:#6b7280;\">\n",
        "      Designed for research labs, security teams, and AI enthusiasts exploring deepfake detection.\n",
        "    </div>\n",
        "  </div>\n",
        "\n",
        "  <div style=\"flex:1.2; width:100%; max-width:720px; margin-top:24px;\">\n",
        "    <div style=\"\n",
        "      position:relative;\n",
        "      border-radius:24px;\n",
        "      padding:18px 20px;\n",
        "      border:1px solid rgba(148,163,184,0.55);\n",
        "      background:linear-gradient(135deg, rgba(15,23,42,0.88), rgba(30,64,175,0.45));\n",
        "      box-shadow:0 30px 60px rgba(15,23,42,0.9);\n",
        "      backdrop-filter:blur(14px);\n",
        "      -webkit-backdrop-filter:blur(14px);\n",
        "      overflow:hidden;\">\n",
        "\n",
        "      <div style=\"\n",
        "        position:absolute;\n",
        "        inset:-40%;\n",
        "        background:radial-gradient(circle at top left, rgba(94,234,212,0.28), transparent 60%);\n",
        "        opacity:0.9;\n",
        "        pointer-events:none;\">\n",
        "      </div>\n",
        "\n",
        "      <div style=\"position:relative; z-index:1;\">\n",
        "        <div style=\"font-size:0.8rem; text-transform:uppercase;\n",
        "                    letter-spacing:0.15em; color:#9ca3af;\">\n",
        "          Live detector snapshot\n",
        "        </div>\n",
        "\n",
        "        <div style=\"margin-top:10px; font-size:1.1rem; color:#e5e7eb; font-weight:600;\">\n",
        "          Your deepfake lab, at a glance\n",
        "        </div>\n",
        "\n",
        "        <div style=\"margin-top:14px; display:grid; grid-template-columns:1fr 1fr; gap:10px; font-size:0.82rem;\">\n",
        "          <div style=\"padding:10px 12px; border-radius:14px;\n",
        "                      background:rgba(15,23,42,0.95); border:1px solid rgba(74,222,128,0.4);\">\n",
        "            <div style=\"color:#6ee7b7; font-size:0.75rem; text-transform:uppercase; letter-spacing:0.08em;\">\n",
        "              Detector mode\n",
        "            </div>\n",
        "            <div style=\"margin-top:4px; font-weight:600;\">Face authenticity</div>\n",
        "            <div style=\"margin-top:2px; color:#9ca3af;\">Vision transformer + symmetry</div>\n",
        "          </div>\n",
        "\n",
        "          <div style=\"padding:10px 12px; border-radius:14px;\n",
        "                      background:rgba(15,23,42,0.95); border:1px solid rgba(96,165,250,0.45);\">\n",
        "            <div style=\"color:#93c5fd; font-size:0.75rem; text-transform:uppercase; letter-spacing:0.08em;\">\n",
        "              Focus\n",
        "            </div>\n",
        "            <div style=\"margin-top:4px; font-weight:600;\">Face swaps & edits</div>\n",
        "            <div style=\"margin-top:2px; color:#9ca3af;\">GAN, diffusion, classic fakes</div>\n",
        "          </div>\n",
        "\n",
        "          <div style=\"padding:10px 12px; border-radius:14px;\n",
        "                      background:rgba(15,23,42,0.95); border:1px solid rgba(244,114,182,0.5); grid-column:span 2;\">\n",
        "            <div style=\"color:#f9a8d4; font-size:0.75rem; text-transform:uppercase; letter-spacing:0.08em;\">\n",
        "              Tip\n",
        "            </div>\n",
        "            <div style=\"margin-top:4px; color:#e5e7eb;\">\n",
        "              Upload a known real and a suspected fake in the <b>Compare</b> tab to see how their\n",
        "              symmetry and texture signatures differ.\n",
        "            </div>\n",
        "          </div>\n",
        "        </div>\n",
        "      </div>\n",
        "    </div>\n",
        "  </div>\n",
        "</div>\n",
        "\"\"\"\n",
        "                )\n",
        "\n",
        "        # PAGE 2 ‚Äì Upload (single image)\n",
        "        with gr.Tab(\"Upload\", id=1):\n",
        "            with gr.Column(elem_classes=\"main-card\"):\n",
        "                gr.Markdown(\"üìÅ Upload Image\", elem_classes=\"section-title\")\n",
        "                gr.Markdown(\n",
        "                    \"Choose a JPG or PNG image and preview it before analysis.\",\n",
        "                    elem_classes=\"section-subtitle\",\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        upload_file = gr.File(\n",
        "                            label=\"Choose Image\",\n",
        "                            file_types=[\".png\", \".jpg\", \".jpeg\"],\n",
        "                            type=\"filepath\",\n",
        "                            elem_classes=\"upload-box\",\n",
        "                        )\n",
        "                        upload_error = gr.Markdown(visible=False, elem_classes=\"error-box\")\n",
        "                        img_info = gr.Markdown(\"\", visible=True)\n",
        "                        analyse_btn = gr.Button(\n",
        "                            \"üîé ANALYZE NOW\",\n",
        "                            elem_classes=\"pill-button\",\n",
        "                            interactive=False,\n",
        "                        )\n",
        "                        back_home1 = gr.Button(\"‚¨Ö Back\", elem_classes=\"secondary-pill\")\n",
        "                    with gr.Column():\n",
        "                        preview = gr.Image(\n",
        "                            label=\"Image Preview\",\n",
        "                            type=\"pil\",\n",
        "                            interactive=False,\n",
        "                            visible=False,\n",
        "                            elem_classes=\"preview-panel\",\n",
        "                        )\n",
        "\n",
        "        # PAGE 3 ‚Äì Compare two images (styled)\n",
        "        with gr.Tab(\"Compare\", id=2):\n",
        "            with gr.Column(elem_classes=\"main-card\"):\n",
        "                gr.Markdown(\"üß¨ Compare Two Images\", elem_classes=\"section-title\")\n",
        "                gr.Markdown(\n",
        "                    \"Upload any two images to see how similar or different the deepfake signals are.\",\n",
        "                    elem_classes=\"section-subtitle\",\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        gr.Markdown(\"### üîπ Image A\")\n",
        "                        pair_a = gr.Image(type=\"pil\", show_label=False)\n",
        "                    with gr.Column():\n",
        "                        gr.Markdown(\"### üî∏ Image B\")\n",
        "                        pair_b = gr.Image(type=\"pil\", show_label=False)\n",
        "\n",
        "                compare_error = gr.Markdown(visible=False, elem_classes=\"error-box\")\n",
        "                compare_btn = gr.Button(\"üîç ANALYZE PAIR\", elem_classes=\"pill-button\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        pair_summary = gr.Markdown(\n",
        "                            visible=False,\n",
        "                            elem_classes=\"details-box\",\n",
        "                            label=\"Pair summary\",\n",
        "                        )\n",
        "                    with gr.Column():\n",
        "                        pair_details = gr.Markdown(\n",
        "                            visible=False,\n",
        "                            elem_classes=\"details-box\",\n",
        "                            label=\"Per‚Äëimage details\",\n",
        "                        )\n",
        "\n",
        "        # PAGE 4 ‚Äì Result (single image)\n",
        "        with gr.Tab(\"Result\", id=3):\n",
        "            with gr.Column(elem_classes=\"main-card\"):\n",
        "                gr.Markdown(\"üß™ Detection Result\", elem_classes=\"section-title\")\n",
        "                gr.Markdown(\n",
        "                    \"See how the model interpreted this image.\",\n",
        "                    elem_classes=\"section-subtitle\",\n",
        "                )\n",
        "                result_banner = gr.HTML()\n",
        "                result_scores = gr.HTML()\n",
        "                result_details = gr.Markdown(elem_classes=\"details-box\")\n",
        "                with gr.Row():\n",
        "                    again_btn = gr.Button(\"üîÅ Analyze Another\", elem_classes=\"pill-button\")\n",
        "                    back_home2 = gr.Button(\"‚¨Ö Back\", elem_classes=\"secondary-pill\")\n",
        "\n",
        "        # PAGE 5 ‚Äì About\n",
        "        with gr.Tab(\"About\", id=4):\n",
        "            with gr.Column(elem_classes=\"main-card\"):\n",
        "                gr.Markdown(\"‚Ñπ Model & Project Lab\", elem_classes=\"section-title\")\n",
        "                gr.Markdown(\n",
        "                    \"Explore how this deepfake detector works under the hood.\",\n",
        "                    elem_classes=\"section-subtitle\",\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    gr.HTML(\n",
        "                        \"\"\"\n",
        "<div style=\"flex:1; padding:14px 16px; border-radius:16px;\n",
        "            background:radial-gradient(circle at top,#1f2937,#020617);\n",
        "            border:1px solid rgba(148,163,184,0.6); color:#e5e7eb; font-size:0.9rem;\">\n",
        "  <div style=\"font-size:0.8rem; text-transform:uppercase; letter-spacing:0.08em; color:#9ca3af;\">\n",
        "    Model core\n",
        "  </div>\n",
        "  <div style=\"font-size:1.05rem; font-weight:600; margin-top:4px;\">\n",
        "    DeiT-small Transformer\n",
        "  </div>\n",
        "  <div style=\"font-size:0.8rem; margin-top:6px; color:#9ca3af;\">\n",
        "    Lightweight vision transformer trained for image understanding.\n",
        "  </div>\n",
        "</div>\n",
        "\"\"\"\n",
        "                    )\n",
        "                    gr.HTML(\n",
        "                        \"\"\"\n",
        "<div style=\"flex:1; padding:14px 16px; border-radius:16px;\n",
        "            background:radial-gradient(circle at top,#052e16,#020617);\n",
        "            border:1px solid rgba(34,197,94,0.5); color:#e5e7eb; font-size:0.9rem;\">\n",
        "  <div style=\"font-size:0.8rem; text-transform:uppercase; letter-spacing:0.08em; color:#86efac;\">\n",
        "    Extra features\n",
        "  </div>\n",
        "  <div style=\"font-size:1.05rem; font-weight:600; margin-top:4px;\">\n",
        "    50‚ÄëD symmetry vector\n",
        "  </div>\n",
        "  <div style=\"font-size:0.8rem; margin-top:6px; color:#bbf7d0;\">\n",
        "    Summarises 68 facial landmarks into geometric imbalance scores.\n",
        "  </div>\n",
        "</div>\n",
        "\"\"\"\n",
        "                    )\n",
        "\n",
        "                gr.HTML(\n",
        "                    \"\"\"\n",
        "<div style=\"margin-top:22px; border-radius:18px;\n",
        "            border:1px solid rgba(148,163,184,0.4); background:#020617; padding:16px 18px;\">\n",
        "  <details open>\n",
        "    <summary style=\"cursor:pointer; font-weight:600; color:#e5e7eb;\">\n",
        "      üîß Architecture walkthrough\n",
        "    </summary>\n",
        "    <div style=\"margin-top:10px; font-size:0.9rem; color:#e5e7eb;\">\n",
        "      <ol style=\"padding-left:18px; line-height:1.5; margin-top:6px;\">\n",
        "        <li><b>Backbone:</b> Image ‚Üí DeiT-small vision transformer.</li>\n",
        "        <li><b>Landmarks:</b> dlib predicts 68 keypoints ‚Üí 50‚Äëdim symmetry vector.</li>\n",
        "        <li><b>Fusion:</b> CLS token + symmetry vector ‚Üí single fused feature.</li>\n",
        "        <li><b>Classifier:</b> Fully‚Äëconnected layer ‚Üí REAL / FAKE logits.</li>\n",
        "      </ol>\n",
        "    </div>\n",
        "  </details>\n",
        "</div>\n",
        "\"\"\"\n",
        "                )\n",
        "\n",
        "    # ---------- navigation & actions ----------\n",
        "    start_btn.click(fn=lambda: go_tab(1), outputs=tabs)\n",
        "\n",
        "    upload_file.upload(\n",
        "        fn=handle_upload,\n",
        "        inputs=upload_file,\n",
        "        outputs=[upload_error, preview, analyse_btn, preview, img_info],\n",
        "    )\n",
        "\n",
        "    analyse_btn.click(\n",
        "        fn=analyse_and_go,\n",
        "        inputs=preview,\n",
        "        outputs=[upload_error, result_banner, result_scores, result_details, tabs],\n",
        "    )\n",
        "\n",
        "    again_btn.click(fn=lambda: go_tab(1), outputs=tabs)\n",
        "    back_home1.click(fn=lambda: go_tab(0), outputs=tabs)\n",
        "    back_home2.click(fn=lambda: go_tab(0), outputs=tabs)\n",
        "\n",
        "    compare_btn.click(\n",
        "        fn=analyse_pair,\n",
        "        inputs=[pair_a, pair_b],\n",
        "        outputs=[compare_error, pair_summary, pair_details],\n",
        "    )\n",
        "\n",
        "print(\"\\nüöÄ Starting Gradio server‚Ä¶\")\n",
        "demo.launch(share=True, debug=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "ozz6_MajwC-t",
        "outputId": "8d8c5f07-4c0a-4938-c135-2054c20de71d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Symmetry extractor ready. SYM_DIM = 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DeiT backbone...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type vit to instantiate a model of type deit. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of DeiTModel were not initialized from the model checkpoint at facebook/deit-small-patch16-224 and are newly initialized: ['embeddings.cls_token', 'embeddings.distillation_token', 'embeddings.patch_embeddings.projection.bias', 'embeddings.patch_embeddings.projection.weight', 'embeddings.position_embeddings', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'layernorm.bias', 'layernorm.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Trained model loaded from /content/drive/MyDrive/Deepfake/deit_fusion_epoch5.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-210091838.py:468: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(title=\"Deepfake Detection\", css=custom_css) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Starting Gradio server‚Ä¶\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://517fb53e0ce95e856a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://517fb53e0ce95e856a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}